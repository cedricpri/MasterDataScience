{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning I exercise done by Cedric Prieels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using convnets with small datasets\n",
    "\n",
    "The first thing to do is download the following file https://lara.web.cern.ch/lara/train.zip in the jupyter terminal and uncompress it in the same folder as this notebook. \n",
    "\n",
    "To download another dataset from imagenet you can do it with the URL list of the images and using `wget -i`\n",
    "\n",
    "\n",
    "\n",
    "## Training a convnet from scratch\n",
    "\n",
    "Training an image classification model with very little data is a common situation you will find yourself in if you end up doing Computer Vision in a professional context.  \n",
    "\n",
    "Having \"few\" samples can mean anything from a few hundred to a few tens of thousands of images.  Let's illustrate a practical example here: let's focus on classifying images as \"dogs\" and \"cats\". \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The importance of DEEP Learning in problems with few data\n",
    "\n",
    "You may have heard many times that Deep Learning only works when you have large amounts of data. This is partly true: one of the characteristics of Deep learning is that you can find interesting features from the training dataset itself, and this a priori is easier when you have many examples available, especially in the case of having input datasets with a high dimensionality, such as images.\n",
    "\n",
    "However, what constitutes a \"large\" dataset is relative. Specifically relative to the size and depth of the network we are trying to train. It is not possible to sand a convnet so that it becomes a complete problem with only a few dozen examples, but a few hundred can be enough if the model is well assembled (we will understand what \"well assembled\" means throughout the Deep Learning course).\n",
    "\n",
    "Since convnets learn local characteristics, invariant under translations, they are very efficient in terms of the number of images needed to carry out perceptual problems. So training a convnet from 0 with a not very large dataset can still lead to reasonable results as we will see here.\n",
    "\n",
    "But there is more: Deep Learning models are highly \"recyclable\". One can, for example, take an image classification problem and a trained speech-to-text converter on a very big dataset and then reuse it for solving a completely different problem only by adding some small modifications. More specifically, in the case of Computer Vision, many pre-trained models (usually trained on the ImageNet dataset) are made public so that one can download them and use them to create powerful Computer Vision models with very little data.\n",
    "\n",
    "But here we will just run a simple example.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los datos\n",
    "\n",
    "The cat vs dog dataset we use is not a Keras package. It was posted on Kaggle.com as part of a Computer Vision problem in late 2013, when ConvNets were not yet so popular. \n",
    "\n",
    "The images are medium resolution JPGEs. It looks like this:\n",
    "\n",
    "![cats_vs_dogs_samples](https://s3.amazonaws.com/book.keras.io/img/ch5/cats_vs_dogs_samples.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's no surprise that the 2013 Kaggle cat vs dog competition was won by ConvNets. The best were able to achieve up to 95% accuracy. In our example we are still far from this accuracy, but during the Deep Learning course we have learned how to approach this value using different methods to improve the performance of neural networks. It should be noted that in this example we are training on approximately only 10% of the data that was used for the contest. \n",
    "After downloading the dataset and decompressing it, we are going to create a new dataset containing three subsets: a training set containing 1000 images of each class, a validation set with 500 images of each class, and finally a test set with 500 images of each class.\n",
    "\n",
    "Here we have a few lines of code that make us this distribution automatically:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The path to the directory where the original\n",
    "# dataset was uncompressed\n",
    "original_dataset_dir = 'train'\n",
    "\n",
    "# The directory where we will\n",
    "# store our smaller dataset\n",
    "base_dir = 'cats_and_dogs_small'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "# Directories for our training,\n",
    "# validation and test splits\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_cats_dir = os.path.join(train_dir, 'cats')\n",
    "os.mkdir(train_cats_dir)\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
    "os.mkdir(train_dogs_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
    "os.mkdir(validation_cats_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_dogs_dir = os.path.join(validation_dir, 'dogs')\n",
    "os.mkdir(validation_dogs_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "test_cats_dir = os.path.join(test_dir, 'cats')\n",
    "os.mkdir(test_cats_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "test_dogs_dir = os.path.join(test_dir, 'dogs')\n",
    "os.mkdir(test_dogs_dir)\n",
    "\n",
    "# Copy first 1000 cat images to train_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy next 500 cat images to validation_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 cat images to test_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_cats_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy first 1000 dog images to train_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to validation_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to test_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_dogs_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's count how many pictures we have in each training split (train/validation/test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training cat images: 1000\n"
     ]
    }
   ],
   "source": [
    "print('total training cat images:', len(os.listdir(train_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training dog images: 1000\n"
     ]
    }
   ],
   "source": [
    "print('total training dog images:', len(os.listdir(train_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation cat images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total validation cat images:', len(os.listdir(validation_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation dog images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test cat images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total test cat images:', len(os.listdir(test_cats_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test dog images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total test dog images:', len(os.listdir(test_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So effectively we have 2000 training images, 1000 validation images and 1000 test images. In each of these subsets there are the same number of examples from each class: this is what is called a balanced binary classification system, which means that our classification accuracy will be an adequate metric of the success of our solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our network\n",
    "\n",
    "In the above example we have built a small convnet to solve the problem of classifying handwritten numbers using the MNIST dataset, so we are already familiar with the terminology that keras uses. We are going to reuse the general structure we had in the previous example: our convnet will have a stack of alternate layers of `Conv2D` (with `relu` activation) and `MaxPooling2D` layers.\n",
    "\n",
    "However, since we are dealing with larger images and a more complex problem, we will create our network accordingly: it will have one more layer of `Conv2D` + `MaxPooling2D`. This serves to increase the capacity of the network and to further reduce the size of the feature maps, so that they are not so huge when they reach the flattening step. We start using 150x150 input images (an arbitrary choice), and end up with feature maps that are 7x7 in size before the flattening layer.\n",
    "\n",
    "It is important to note that the depth of feature maps progressively increases as we move through the neural network (from 32 to 128) while the size of feature maps decreases (from 148x148 to 7x7). You will see this pattern in almost all convnets.\n",
    "\n",
    "As we are attacking a binary classification problem (dog or cat), we are going to finish the network with a single unit (a dense layer of size 1) and with a sigmoid activation. This unit will encode the probability that our network is looking at one class or another.\n",
    "\n",
    "The final look of the model should be as follows:\n",
    "\n",
    "\n",
    "![modelo_red_animales.png](https://github.com/laramaktub/MachineLearningI/blob/master/modelo_red_animales.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.layers import Dense, Activation, Flatten\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3))) #T150x150px dataset input, 3 colors\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='linear'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "#Next step\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary() #Check the results, we now have more than 3 million parameters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el paso de compilación utilizaremos el optimizador `RMSprop`(lr=1e-4). Como nuestra red termina con una única unidad sigmoide, vamos a utilizar binary crossentropy como nuestra función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "model.compile(optimizer=optimizers.rmsprop(lr=1e-4), loss=\"binary_crossentropy\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Data preprocessing\n",
    "\n",
    "The images must be properly formatted as float tensors before they are given to the net. That's just what we're going to do here. Before we pre-process them, the images are JPEG files. The steps to be able to give them to our network are roughly as follows:\n",
    "\n",
    "* Read the files with the images.\n",
    "* Decode the content of the JPEG in a \"grid\" with the RGB of the pixels \n",
    "* Turn that \"grill\" into floatation devices\n",
    "* Rescale the pixel values (between 0 and 255) to the [0, 1] interval as neural networks prefer to work with small values. \n",
    "\n",
    "All this may seem very complicated but thanks to Keras our life is much easier and we can count on your tools to take care of these steps automatically. Keras has a module with tools for image processing, which can be found in `keras.preprocessing.image`. In particular, it contains the class `ImageDataGenerator` that allows us to automatically convert images we have on the hard disk into pre-processed tensors. This is exactly what we'll be using next. To do this we can use the flow_from_directory to take the images directly from the folders that we previously generated. We give it as input the folders where the training (or validation) images are, the size of the images (target_size), the size of the batch we're going to use (we're going to start with 20) and as there are only two categories, we tell it that we're going to use binary_crossentropy (class_mode). When we run these commands we'll get the following, the total number of images and how many classes there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator()\n",
    "test_datagen = ImageDataGenerator()\n",
    "validation_datagen = ImageDataGenerator()\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(directory=\"cats_and_dogs_small/train/\", target_size=(150, 150), batch_size=20, class_mode='binary')\n",
    "test_generator = test_datagen.flow_from_directory(directory=\"cats_and_dogs_small/test/\", target_size=(150, 150), batch_size=20, class_mode='binary')\n",
    "validation_generator = validation_datagen.flow_from_directory(directory=\"cats_and_dogs_small/validation\", target_size=(150, 150), batch_size=20, class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at one of these generators: it takes us to a batch of 150x150 RGB images (dimensions `(20, 150, 150, 3)`) and binary tags (dimension `(20,)`). 20 is the number of examples in each batch (what we call the batch size). The generator generates these batches indefinitely: it runs a loop endlessly through all the images we have in the folder. That's why we have to type `break` to break the loop at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: (20,)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make the fit. In this case, as what we have is a generator, we use fit_generator. We are going to run 30 epochs and use the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 83s 827ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 83s 831ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 82s 817ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 81s 814ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 121s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 122s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 121s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 121s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 120s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 120s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 118s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 120s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 120s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 121s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 122s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 120s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 121s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 120s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 120s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 122s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 120s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 121s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 119s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 121s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 122s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 121s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 122s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 120s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 121s 1s/step - loss: 7.9712 - acc: 0.5000 - val_loss: 7.9712 - val_acc: 0.5000\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator, epochs=30, validation_data=validation_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "It's a nice idea to save the model after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('net_numbers.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now evaluate our model using the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss =  7.9711928558349605\n",
      "Accuracy =  0.5000000014901161\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate_generator(test_generator)\n",
    "print(\"Loss = \", scores[0])\n",
    "print(\"Accuracy = \", scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to optimize the network with the tools you have learnt during the lesson. Try to make improvements both in terms of speed and accuracy. Comment the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw previously, the model that we considered was far from great since its accuracy was low (only 50% for two categories, the same as guessing) and it was that it was not able to learn, since the accuracy and the loss stayed constant. For this exercise, in order to reduce the number of hyperparameters to optimize, we will not change the model previously defined by adding/removing layers/neurons, we are just going to play with the fitting and training parameters (mainly the learning rate of the optimizer, the rescale parameter, the batch size and the parameters related to the fit itself). Let's also make this process a bit more automatic by defining a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_weights(model):\n",
    "    import keras.backend as K\n",
    "    session = K.get_session()\n",
    "    for layer in model.layers: \n",
    "        if hasattr(layer, 'kernel_initializer'): \n",
    "            layer.kernel.initializer.run(session=session)\n",
    "        if hasattr(layer, 'bias_initializer'):\n",
    "            layer.bias.initializer.run(session=session)  \n",
    "\n",
    "def trainAndEvaluate(model, learningRate = 1e-4, rescale = False, batchSize = 20, epochs = 30, name=\"myModel\"):\n",
    "    #First, reset the weights of the model\n",
    "    reset_weights(model)\n",
    "    \n",
    "    model.compile(optimizer=optimizers.rmsprop(lr=1e-4), loss=\"binary_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "    if rescale:\n",
    "        train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "        test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "        validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    else:\n",
    "        train_datagen = ImageDataGenerator()\n",
    "        test_datagen = ImageDataGenerator()\n",
    "        validation_datagen = ImageDataGenerator()\n",
    "        \n",
    "    train_generator = train_datagen.flow_from_directory(directory=\"cats_and_dogs_small/train/\", target_size=(150, 150), batch_size=batchSize, class_mode='binary')\n",
    "    test_generator = test_datagen.flow_from_directory(directory=\"cats_and_dogs_small/test/\", target_size=(150, 150), batch_size=batchSize, class_mode='binary')\n",
    "    validation_generator = validation_datagen.flow_from_directory(directory=\"cats_and_dogs_small/validation\", target_size=(150, 150), batch_size=batchSize, class_mode='binary')\n",
    "    \n",
    "    history = model.fit_generator(train_generator, epochs=epochs, validation_data=validation_generator)\n",
    "    model.save(str(name)+'.h5')\n",
    "    \n",
    "    scores = model.evaluate_generator(test_generator)\n",
    "    print(\"Loss = \", scores[0])\n",
    "    print(\"Accuracy = \", scores[1])\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try by simply using the rescale parameters of the ImageDataGenerator(), since it seems that it does improve a lot the results obtained. Now, our model is able to learn but we can clearly see that a constant accuracy with the validation dataset is already obtain after around 10 epochs, so let's use this value from now on for this exercise in order to speed up a bit the training and avoid overtraining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Epoch 1/30\n",
      "100/100 [==============================] - 83s 835ms/step - loss: 0.6875 - acc: 0.5430 - val_loss: 0.6646 - val_acc: 0.5910\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 81s 812ms/step - loss: 0.6412 - acc: 0.6265 - val_loss: 0.6213 - val_acc: 0.6540\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 81s 815ms/step - loss: 0.5813 - acc: 0.6905 - val_loss: 0.6114 - val_acc: 0.6510\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 81s 812ms/step - loss: 0.5389 - acc: 0.7220 - val_loss: 0.5719 - val_acc: 0.6960\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 81s 811ms/step - loss: 0.5049 - acc: 0.7460 - val_loss: 0.5761 - val_acc: 0.6980\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 82s 819ms/step - loss: 0.4721 - acc: 0.7775 - val_loss: 0.5741 - val_acc: 0.7010\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 83s 828ms/step - loss: 0.4324 - acc: 0.7945 - val_loss: 0.5585 - val_acc: 0.7030\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 83s 831ms/step - loss: 0.3980 - acc: 0.8205 - val_loss: 0.5614 - val_acc: 0.7030\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 83s 833ms/step - loss: 0.3628 - acc: 0.8425 - val_loss: 0.5726 - val_acc: 0.7220\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 83s 831ms/step - loss: 0.3250 - acc: 0.8615 - val_loss: 0.6262 - val_acc: 0.7090\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 83s 832ms/step - loss: 0.2876 - acc: 0.8925 - val_loss: 0.6212 - val_acc: 0.7220\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 82s 823ms/step - loss: 0.2560 - acc: 0.9045 - val_loss: 0.6130 - val_acc: 0.7460\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 83s 835ms/step - loss: 0.2236 - acc: 0.9215 - val_loss: 0.6405 - val_acc: 0.7320\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 83s 831ms/step - loss: 0.1897 - acc: 0.9355 - val_loss: 0.6385 - val_acc: 0.7430\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 83s 834ms/step - loss: 0.1647 - acc: 0.9435 - val_loss: 0.6909 - val_acc: 0.7350\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 84s 844ms/step - loss: 0.1343 - acc: 0.9635 - val_loss: 0.7188 - val_acc: 0.7430\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 85s 846ms/step - loss: 0.1077 - acc: 0.9695 - val_loss: 0.7879 - val_acc: 0.7230\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 85s 847ms/step - loss: 0.0844 - acc: 0.9750 - val_loss: 0.8045 - val_acc: 0.7290\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 84s 835ms/step - loss: 0.0675 - acc: 0.9850 - val_loss: 0.9133 - val_acc: 0.7120\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 85s 847ms/step - loss: 0.0512 - acc: 0.9915 - val_loss: 0.9017 - val_acc: 0.7240\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 84s 844ms/step - loss: 0.0373 - acc: 0.9930 - val_loss: 0.9513 - val_acc: 0.7270\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 84s 838ms/step - loss: 0.0287 - acc: 0.9950 - val_loss: 0.9859 - val_acc: 0.7270\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 85s 852ms/step - loss: 0.0211 - acc: 0.9965 - val_loss: 1.1415 - val_acc: 0.7250\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 86s 862ms/step - loss: 0.0194 - acc: 0.9975 - val_loss: 1.1376 - val_acc: 0.7260\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 90s 902ms/step - loss: 0.0118 - acc: 0.9995 - val_loss: 1.3019 - val_acc: 0.7250\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 93s 931ms/step - loss: 0.0082 - acc: 1.0000 - val_loss: 1.2656 - val_acc: 0.7230\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 94s 941ms/step - loss: 0.0077 - acc: 0.9990 - val_loss: 1.2375 - val_acc: 0.7300\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 92s 923ms/step - loss: 0.0087 - acc: 0.9970 - val_loss: 1.3536 - val_acc: 0.7240\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 93s 933ms/step - loss: 0.0045 - acc: 0.9995 - val_loss: 1.4001 - val_acc: 0.7320\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 91s 912ms/step - loss: 0.0040 - acc: 0.9990 - val_loss: 1.4792 - val_acc: 0.7230\n",
      "Loss =  1.5449113744497298\n",
      "Accuracy =  0.7090000009536743\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5449113744497298, 0.7090000009536743]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainAndEvaluate(model, learningRate = 1e-4, rescale = True, batchSize = 20, epochs = 30, name=\"myModel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try to change the learning rate to see if it changes anything. It does not seem to have much of an impact anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 89s 887ms/step - loss: 0.6864 - acc: 0.5515 - val_loss: 0.6616 - val_acc: 0.6140\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 92s 919ms/step - loss: 0.6410 - acc: 0.6265 - val_loss: 0.6216 - val_acc: 0.6590\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 90s 895ms/step - loss: 0.5776 - acc: 0.6905 - val_loss: 0.5998 - val_acc: 0.6730\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 86s 857ms/step - loss: 0.5369 - acc: 0.7215 - val_loss: 0.5942 - val_acc: 0.6900\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 85s 847ms/step - loss: 0.5013 - acc: 0.7540 - val_loss: 0.6038 - val_acc: 0.6780\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 87s 869ms/step - loss: 0.4731 - acc: 0.7740 - val_loss: 0.5771 - val_acc: 0.7010\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 87s 874ms/step - loss: 0.4417 - acc: 0.7945 - val_loss: 0.5685 - val_acc: 0.7150\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 87s 867ms/step - loss: 0.4046 - acc: 0.8190 - val_loss: 0.5931 - val_acc: 0.7010\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 86s 861ms/step - loss: 0.3676 - acc: 0.8450 - val_loss: 0.5808 - val_acc: 0.7090\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 86s 859ms/step - loss: 0.3311 - acc: 0.8600 - val_loss: 0.6055 - val_acc: 0.7190\n",
      "Loss =  0.5923161289095878\n",
      "Accuracy =  0.7099999988079071\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "100/100 [==============================] - 86s 860ms/step - loss: 0.6873 - acc: 0.5425 - val_loss: 0.6772 - val_acc: 0.5620\n",
      "Epoch 2/10\n",
      "100/100 [==============================] - 85s 851ms/step - loss: 0.6415 - acc: 0.6270 - val_loss: 0.6271 - val_acc: 0.6360\n",
      "Epoch 3/10\n",
      "100/100 [==============================] - 83s 832ms/step - loss: 0.5818 - acc: 0.6835 - val_loss: 0.5987 - val_acc: 0.6760\n",
      "Epoch 4/10\n",
      "100/100 [==============================] - 85s 850ms/step - loss: 0.5471 - acc: 0.7225 - val_loss: 0.6106 - val_acc: 0.6450\n",
      "Epoch 5/10\n",
      "100/100 [==============================] - 84s 839ms/step - loss: 0.5191 - acc: 0.7315 - val_loss: 0.6057 - val_acc: 0.6700\n",
      "Epoch 6/10\n",
      "100/100 [==============================] - 85s 847ms/step - loss: 0.4881 - acc: 0.7620 - val_loss: 0.6423 - val_acc: 0.6570\n",
      "Epoch 7/10\n",
      "100/100 [==============================] - 85s 846ms/step - loss: 0.4595 - acc: 0.7775 - val_loss: 0.5920 - val_acc: 0.6760\n",
      "Epoch 8/10\n",
      "100/100 [==============================] - 84s 837ms/step - loss: 0.4282 - acc: 0.7995 - val_loss: 0.5758 - val_acc: 0.6880\n",
      "Epoch 9/10\n",
      "100/100 [==============================] - 85s 851ms/step - loss: 0.3932 - acc: 0.8225 - val_loss: 0.5826 - val_acc: 0.6960\n",
      "Epoch 10/10\n",
      "100/100 [==============================] - 86s 857ms/step - loss: 0.3583 - acc: 0.8460 - val_loss: 0.5862 - val_acc: 0.7180\n",
      "Loss =  0.5863171517848969\n",
      "Accuracy =  0.7120000016689301\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5863171517848969, 0.7120000016689301]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainAndEvaluate(model, learningRate = 1e-5, rescale = True, batchSize = 20, epochs = 10, name=\"myModel2\")\n",
    "trainAndEvaluate(model, learningRate = 1e-3, rescale = True, batchSize = 20, epochs = 10, name=\"myModel3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also try to increase/decrease the batchSize. A lower batchsize increase by ~20% the training time and does not improve the accuracy. A batch size of 50, on the other hand, lowers the training time by ~10% while keeping a similar accuracy (even though slightly smaller)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "200/200 [==============================] - 97s 487ms/step - loss: 0.6837 - acc: 0.5555 - val_loss: 0.6745 - val_acc: 0.5670\n",
      "Epoch 2/10\n",
      "200/200 [==============================] - 96s 482ms/step - loss: 0.6147 - acc: 0.6635 - val_loss: 0.6065 - val_acc: 0.6630\n",
      "Epoch 3/10\n",
      "200/200 [==============================] - 96s 478ms/step - loss: 0.5502 - acc: 0.7200 - val_loss: 0.6071 - val_acc: 0.6670\n",
      "Epoch 4/10\n",
      "200/200 [==============================] - 96s 481ms/step - loss: 0.4971 - acc: 0.7460 - val_loss: 0.6140 - val_acc: 0.6830\n",
      "Epoch 5/10\n",
      "200/200 [==============================] - 97s 484ms/step - loss: 0.4472 - acc: 0.7940 - val_loss: 0.5780 - val_acc: 0.7050\n",
      "Epoch 6/10\n",
      "200/200 [==============================] - 97s 483ms/step - loss: 0.3906 - acc: 0.8250 - val_loss: 0.6028 - val_acc: 0.7060\n",
      "Epoch 7/10\n",
      "200/200 [==============================] - 97s 483ms/step - loss: 0.3440 - acc: 0.8510 - val_loss: 0.6402 - val_acc: 0.7100\n",
      "Epoch 8/10\n",
      "200/200 [==============================] - 98s 490ms/step - loss: 0.3045 - acc: 0.8755 - val_loss: 0.6097 - val_acc: 0.7120\n",
      "Epoch 9/10\n",
      "200/200 [==============================] - 104s 519ms/step - loss: 0.2460 - acc: 0.9025 - val_loss: 0.6579 - val_acc: 0.7180\n",
      "Epoch 10/10\n",
      "200/200 [==============================] - 108s 539ms/step - loss: 0.1973 - acc: 0.9250 - val_loss: 0.7141 - val_acc: 0.7150\n",
      "Loss =  0.6933748013526201\n",
      "Accuracy =  0.7090000005066395\n",
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n",
      "Epoch 1/10\n",
      "40/40 [==============================] - 77s 2s/step - loss: 0.6906 - acc: 0.5480 - val_loss: 0.6740 - val_acc: 0.5850\n",
      "Epoch 2/10\n",
      "40/40 [==============================] - 73s 2s/step - loss: 0.6620 - acc: 0.6010 - val_loss: 0.6480 - val_acc: 0.6350\n",
      "Epoch 3/10\n",
      "40/40 [==============================] - 81s 2s/step - loss: 0.6215 - acc: 0.6635 - val_loss: 0.6275 - val_acc: 0.6250\n",
      "Epoch 4/10\n",
      "40/40 [==============================] - 80s 2s/step - loss: 0.5796 - acc: 0.7045 - val_loss: 0.5884 - val_acc: 0.6770\n",
      "Epoch 5/10\n",
      "40/40 [==============================] - 78s 2s/step - loss: 0.5518 - acc: 0.7215 - val_loss: 0.5801 - val_acc: 0.6830\n",
      "Epoch 6/10\n",
      "40/40 [==============================] - 78s 2s/step - loss: 0.5250 - acc: 0.7360 - val_loss: 0.5785 - val_acc: 0.6850\n",
      "Epoch 7/10\n",
      "40/40 [==============================] - 82s 2s/step - loss: 0.5061 - acc: 0.7520 - val_loss: 0.5723 - val_acc: 0.7020\n",
      "Epoch 8/10\n",
      "40/40 [==============================] - 89s 2s/step - loss: 0.4866 - acc: 0.7765 - val_loss: 0.5547 - val_acc: 0.7080\n",
      "Epoch 9/10\n",
      "40/40 [==============================] - 95s 2s/step - loss: 0.4595 - acc: 0.7815 - val_loss: 0.5644 - val_acc: 0.7110\n",
      "Epoch 10/10\n",
      "40/40 [==============================] - 91s 2s/step - loss: 0.4512 - acc: 0.7930 - val_loss: 0.5687 - val_acc: 0.7070\n",
      "Loss =  0.5659809440374375\n",
      "Accuracy =  0.6949999958276749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5659809440374375, 0.6949999958276749]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainAndEvaluate(model, learningRate = 1e-4, rescale = True, batchSize = 10, epochs = 10, name=\"myModel4\")\n",
    "trainAndEvaluate(model, learningRate = 1e-4, rescale = True, batchSize = 50, epochs = 10, name=\"myModel5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best parameters obtained for the model defined in this case are 10 training epochs, a learning rate of 1e-4, rescaling the data and a batchsize of 50."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
